{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentingorce/tp_centrale/blob/main/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiy0xFlZLomY"
      },
      "source": [
        "# Web Information Retrieval\n",
        "## Introduction to search engines\n",
        "\n",
        "### DAY 2: Teacher version\n",
        "### Implementing a search engine\n",
        "\n",
        "The goal of this second session is to implement a first architecture of a search engine on the previously introduced dataset (stackexchange-datascience). If you missed the first session or if you did not saved the dataset, please reload the first session's notebook to download it. \n",
        "\n",
        "If you need some ifnormation about the dataset, it should be available here : https://archive.org/details/stackexchange\n",
        "\n",
        "The notebook is divided into several steps:\n",
        "-\tImplement the indexation\n",
        "-\tImplement the search method\n",
        "-\tDefine a ranking strategy and implement it\n",
        "-\tSuggest some improvements of the search engine\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_WrILaSIJL"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H819R7iJF_y"
      },
      "outputs": [],
      "source": [
        "!pip install ttable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lI2VFiG1SJmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tt import BooleanExpression\n",
        "from itertools import product\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YySiGfQ1SNwT"
      },
      "outputs": [],
      "source": [
        "# Only if you use Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0rq6fLsSSPUn"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "DATA_PATH = '' \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kiMuicy6Du8"
      },
      "source": [
        "**Important :**\n",
        "\n",
        "An Excel file for testing the evaluation part is available in the gitlab repo : evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\n",
        "\n",
        "If you work on Colab, we advice you to push it directly on your Google Drive directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzqYFBxYAJN1"
      },
      "source": [
        "# Implement the indexation\n",
        "As you might already know, for a search engine to work properly an index of the documents must be created. Here we will keep it in python, and try to use only common libraries to keep it simple.\n",
        "\n",
        "Once created, the index will be used to match the query with the documents. As a result, there are several ways to build an index, using statistical, boolean, semantic indexation...\n",
        "\n",
        "First of, let's make a naive one that will consist in breaking down each document into a set of the words it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lOClVNL5_abL"
      },
      "outputs": [],
      "source": [
        "def extract_words(text:str)->list:\n",
        "  # TODO\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QwVgveW6CIAz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# test\n",
        "s = \"The cat is sat on the mat. The dog is laid on the mat.\"\n",
        "assert extract_words(s).sort()==[\"The\",\"cat\",\"is\",\"sat\",\"on\",\"the\",\"mat.\",\"dog\",\"laid\"].sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTFNPmNJC75C"
      },
      "source": [
        "As you may notice, there are several problems with the previous implementation. First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "\n",
        "Re-implement this function with some basic preprocessing to avoid these issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eg9aRYX-CTTH"
      },
      "outputs": [],
      "source": [
        "def extract_words(text:str)->list:\n",
        "  #TODO\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wnTSVCA1Fd1q"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "assert extract_words(s).sort()==[\"the\",\"cat\",\"is\",\"sat\",\"on\",\"mat\",\"dog\",\"laid\"].sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQq7QZKF7mM"
      },
      "source": [
        "Now you sould be able to create your index table. For now we will just make a dataframe with two columns: [raw_text, words]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XLO7naGaF0LP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def index_docs(docs:list[str])->pd.DataFrame:\n",
        "  # TODO\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpK3zlftGw_w"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "\n",
        "L = [s, \"Hello World!\", \"Goodbye\", \"How are you?\"]\n",
        "\n",
        "index_docs(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70w-UfpsHktY"
      },
      "source": [
        "Now, let's try it on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46pO8FszG_4s"
      },
      "outputs": [],
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcaGAngHLK4g"
      },
      "source": [
        "For our first version of the indexation mechanism, we will simply use the \"body\" of the posts. To have a better search engine, the title and other metadata aswell could be used aswell. Finally, not all the XML files have a \"body\" feature, so for the search engine to retrieve information from any of the files you will need to implement another way to index.\n",
        "\n",
        "But first, let's start with \"body\". There is more to preprocess than before, indeed, there are html tags such as \"<p>\" for instance. They are not useful for us, because users won't use them in their queries. So we first need to remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Et-7VlXyKuaf"
      },
      "outputs": [],
      "source": [
        "def remove_tags(text:str)->str:\n",
        "  # TODO\n",
        "\n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQAW9pi9MkyZ"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "remove_tags('<p>Hello World!\\nI am making a search engine.<p>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDiFEsPtMszw"
      },
      "outputs": [],
      "source": [
        "clean_posts = posts[['Id','Body']]\n",
        "clean_posts['Clean Body'] = clean_posts['Body'].fillna('').apply(remove_tags)\n",
        "clean_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RbkcTyrNDcJ"
      },
      "outputs": [],
      "source": [
        "clean_posts['words'] = clean_posts['Clean Body'].apply(extract_words)\n",
        "clean_posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfNgdg7ziCm"
      },
      "source": [
        "## Zipf Law\n",
        "\n",
        "A way of analyzing a corpus is to draw the zipf law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yrts6RVNziLk"
      },
      "outputs": [],
      "source": [
        "# TODO : Draw Zipf Law on the Posts Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOYd2qLpwhHE"
      },
      "source": [
        "## Inverted Index\n",
        "\n",
        "Now, we want to go further on the indexing and build an inverted index. Inverted index is a dictionary where the keys are the words of the vocabulary and the values are the documents containing these words. Reducing the size of the vocabulary is a relevant first step when building an inverted index. Here, we will focus on the creation of the index, we leave you the optimisation steps :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-sCV0Ds21g7J"
      },
      "outputs": [],
      "source": [
        "def create_index(posts:pd.DataFrame)-> set:\n",
        "  # TODO\n",
        "  \n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uCPc3tRMZj9R"
      },
      "outputs": [],
      "source": [
        "inverted_index = create_index(clean_posts.iloc[0:5000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q97D2TjBOVZP"
      },
      "source": [
        "#### Well Done, you've indexed the dataset! \n",
        "Don't hesitate to save your indexes in txt or pickle file\n",
        "\n",
        "---\n",
        "# Implement the search method\n",
        "\n",
        "A naive method would be to count the number of words in common between the query and each posts. Then to rank the posts you could directly select the post who maximize the number of common words. Let's implement this approach :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UZX8J3Vrq7St"
      },
      "outputs": [],
      "source": [
        "# Implement the word_in_index function \n",
        "# Inputs : a word (str) & a list of words\n",
        "# Output : pandas series of 1 if the word is in the list, else 0\n",
        "\n",
        "def word_in_index(word, word_list_index):\n",
        "  # TODO\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GFvxO88LtVi8"
      },
      "outputs": [],
      "source": [
        "# Implement the function which run through a pandas series and count the number of word in common\n",
        "# Use extract_words method, apply method with word_in_index function\n",
        "# Inputs : the query (str) & pandas series of strings\n",
        "# Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
        "\n",
        "def count_common_words(query, word_serie):\n",
        "  # TODO\n",
        "  \n",
        "  return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NHzyXeExNWQq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rank_top_query(query, df, top=5):\n",
        "  # TODO\n",
        "\n",
        "\n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRdErltStZGv"
      },
      "outputs": [],
      "source": [
        "rank_top_query(query=\"testing the query in python\", df=clean_posts, top=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JumHiP3txgUb"
      },
      "source": [
        "Testez plusieurs requêtes et critiquez les résultats obtenus.\n",
        "\n",
        "Quels sont les pros and cons de cette méthodes. Vous l'indiquerez sur le rapport avec vos réflexions pour l'améliorer.\n",
        "\n",
        "Next, you have to implement the first improvements you find in the search method to get most relevant results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ux77Xzftx-kX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_stop_words(l_txt: list) -> list:\n",
        "  # TODO\n",
        "\n",
        "  return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MecCCwzbx8qZ"
      },
      "source": [
        "## Boolean Search\n",
        "\n",
        "Thanks to the ttable library, implement a boolean search method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5JTNdIrVdH9"
      },
      "outputs": [],
      "source": [
        "def boolean_search(query):\n",
        "  # TODO\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4vFldsAycpB"
      },
      "source": [
        "## Probabilistic search\n",
        "\n",
        "Implement the MIB or BM25 method of searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCxjyrldyhUB"
      },
      "outputs": [],
      "source": [
        "def probabilistic_search(query):\n",
        "  # TODO\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9m_LFo_yog2"
      },
      "source": [
        "Compare the naive method with your improvements and the boolean and probabilistic search. (report)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arfKMWtLyyxY"
      },
      "source": [
        "# Evaluate the Search\n",
        "\n",
        "Now you implement multiple search methods and you're able to improve it. You have to define metric to compare it objectively.\n",
        "\n",
        "\n",
        "\n",
        "We ask you to implement NDCG (Normalized Discounted Cumulative Gain) from few queries we implement on a dozen of post. We already defined the values of relevance judgement in the xlsx file : . The final score will be the mean quadratic error of the queries.\n",
        "\n",
        "\n",
        "Explication for the xlsx file :\n",
        "\n",
        "We propose you a Excel file with some posts and a mesure of relevancy for the queries\n",
        "\n",
        "- First column is the post Id,\n",
        "- Columns starting by query are the queries you have to test.\n",
        "- The values in this columns are the rank of relevancy of the post in regard with the query.\n",
        "- The missing values indicates you should not take into account the post\n",
        "\n",
        "\n",
        "You will have to criticize this metric and your result in the report. Then you will have to propose some improvements. \n",
        "\n",
        "Thereafter in this week, you will have to compare your different search engines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxCcftBPagMQ"
      },
      "outputs": [],
      "source": [
        "# Read Relevancy CSV\n",
        "df_relevancy = pd.read_excel(\"/content/drive/MyDrive/TP Centrale/evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVxld-Ujy0nN"
      },
      "outputs": [],
      "source": [
        "def calculate_ndgc(query_col=\"query\", output_col=\"query_output\"):\n",
        "  # TODO\n",
        "\n",
        "  return\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
