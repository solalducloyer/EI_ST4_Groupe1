{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 3: Student version"
      ],
      "metadata": {
        "id": "Xycip-y7MmUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning NLP**\n",
        "\n",
        "The goal of this session is to improve the search engine using NLP features.\n",
        "\n",
        "This notebook guides you through different techniques to explore. It is expected of you to be inventive and improve the techniques introduced. \n",
        "\n",
        "First, let's import the useful packages and load the data."
      ],
      "metadata": {
        "id": "gnCKD6NtjZno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "tS0Nv3gMZw7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU8SMVS4Zu0w",
        "outputId": "0d620766-c1ed-4c34-d70a-54f92de506b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "TBamWPKlkZs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from scipy.sparse import find\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "metadata": {
        "id": "50KuGjmPjquG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "bVl7AEcBkgyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only if you use Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# TODO:\n",
        "DATA_PATH = '' \n",
        "\n",
        "# CORR:\n",
        "DATA_PATH = '/content/drive/MyDrive/TP Centrale/data'\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVb0xZIvkevD",
        "outputId": "6e8028c1-4137-45b7-a6da-beebe12a595a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "posts"
      ],
      "metadata": {
        "id": "YBtfx_3RYSs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "2q_ryLonxLYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a function to clean the posts. \n",
        "\n",
        "You can reuse what you have used in the Day 2 notebook or improve it."
      ],
      "metadata": {
        "id": "cnOIzOY4e5Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_post(text:str)->str:\n",
        "    #TODO \n",
        "    return clean_post\n"
      ],
      "metadata": {
        "id": "9e-XxHMAkmEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts['cleaned_body'] = posts.Body.apply(clean_post)"
      ],
      "metadata": {
        "id": "Yv_n2e5Y1a04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also implement a function that cleans the user's query (the query). \n",
        "\n",
        "This step is optionnal (if you don't think that it is necessary, just return the query)"
      ],
      "metadata": {
        "id": "9vYfAFcDe_ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_query(text:str)->str:\n",
        "    #TODO\n",
        "    return cleaned_query"
      ],
      "metadata": {
        "id": "Q8_gvmjXwBwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text specific metadata"
      ],
      "metadata": {
        "id": "BnjdSpeLlZzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What metadata can you get from the text at your disposal ? Which ones are relevant ? "
      ],
      "metadata": {
        "id": "qEyQEpoaoGLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "5b_O0YS-oMAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classic Preprocessing"
      ],
      "metadata": {
        "id": "o0rsRxD4xeli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal for this part is to implement a classic vectorization (Count vectorizer, tfidf...).\n",
        "\n",
        "You can do it on your own or use scikit-learn.\n",
        "\n",
        "Hints : pay attention to stopwords, additionnal preprocessing steps and techniques of vectoriation\n"
      ],
      "metadata": {
        "id": "NLmg2T3qxjc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = #TODO\n",
        "vectorizer.fit(posts.cleaned_body.values)\n",
        "vectors = vectorizer.transform(posts.cleaned_body.values)"
      ],
      "metadata": {
        "id": "VTaWk2nAAiTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that applies the same process to the query\n"
      ],
      "metadata": {
        "id": "drg1x_9v1RD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_query(query : str, vectorizer=vectorizer):\n",
        "    \"\"\"vectorizes the query\n",
        "    Args:\n",
        "        query (str): query string\n",
        "        vectorizer (optional): Defaults to vectorizer.\n",
        "\n",
        "    Returns:\n",
        "        query vectorized\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "\n",
        "    return query_vectorized"
      ],
      "metadata": {
        "id": "RpBr0mfF1QVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine a way to use this vectorization to suggest the closest items to the entry in the database"
      ],
      "metadata": {
        "id": "ZO5uBYas15HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer_search(query : str,\n",
        "                      vectors=vectors,\n",
        "                      vectorizer=vectorizer) -> list:\n",
        "    #TODO\n",
        "    return closest_items"
      ],
      "metadata": {
        "id": "X2JhLiCU1NPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entry = 'what is stochastic gradient descent ?'"
      ],
      "metadata": {
        "id": "FHVHzd-G3Sd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " vectorizer_search(entry)"
      ],
      "metadata": {
        "id": "Ir0QIX-XGE-N",
        "outputId": "2e456d24-c2d8-4a64-b1f6-5c418e081f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"i'm currently working on implementing stochastic gradient descent, sgd, for neural nets using back-propagation, and while i understand its purpose i have some questions about how to choose values for the learning rate.is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?if so, how do you use this information to inform your decision about a value?if it's not what sort of values should i choose, and how should i choose them?it seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend?does it make sense to have a constant learning rate, or should i use some metric to alter its value as i get nearer a minimum in the gradient?in short: how do i choose the learning rate for sgd?\",\n",
              "       'there are several reasons why you are getting erroneous results.first, you should consider using log likelihood instead of likelihood. there are numerical issues with multiplying many small numbers(imagine if you had millions of samples you had to multiply millions of small numbers for the lhd). also taking gradients for optimization methods that require gradients is often easier when you are dealing with the log likelihood. in general, it is good to have an objective which is a sum rather than a product of variables when dealing with optimization problems.second, fmin is using nelder-mead simplex algorithm which has no convergence guarantees according to scipy documentation. this means the convergence is totally random and you should not expect to find parameters close to the originals. to get around this, i would suggest you to use a gradient based method like stochastic gradient descent or bfgs. since you know the generative model (rvs are gaussian distributed) you can write the likelihood and log likelihood as:where a,b,c and d are your model parameters 5,2,3 and 4 respectively.then take the gradient with respect to [a,b,c,d] and feed that into prime input of fmin_bfgs. note that due to varying variance what could be solved by just linear regression is now a nastier problem.finally, you may also want to check generalized least squares here and here, which talk about your problem and offer several available solutions.good luck!',\n",
              "       \"stochastic gradient descent is a method of setting the parameters of the regressor; since the objective for logistic regression is convex (has only one maximum), this won't be an issue and sgd is generally only needed to improve convergence speed with masses of training data.what your numbers suggest to me is that your features are not adequate to separate the classes. consider adding extra features if you can think any any that are useful. you might also consider interactions and quadratic features in your original feature space.\",\n",
              "       'gradient boosting is also a good choice here.  you can use the gradient boosting classifier in sci-kit learn for example.  gradient boosting is a principled method of dealing with class imbalance by constructing successive training sets based on incorrectly classified examples.',\n",
              "       'r base function glm() uses fishers scoring for mle, while the glmnet appears to use the coordinate descent method to solve the same equation. coordinate descent is more time-efficient than fisher scoring, as fisher scoring calculates the second order derivative matrix, in addition to some other matrix operations. which makes expensive to perform, while coordinate descent can do the same task in o(np) time.why would r base function use fisher scoring? does this method have an advantage over other optimization methods? how does coordinate descent and fisher scoring compare? i am relatively new to do this field so any help or resource will be helpful.',\n",
              "       'conjugate gradient -- the cg in function minimization (nonlinear) conjugate gradiant -- requires you to have a gradient function (or approximation) since that is a critical part of the algorithm itself: it needs to find the steepest descent direction quickly.fminsearch implements nelder-mead, a nonlinear gradient-free method. its convergence properties are not anywhere near as good.what is your cost function? are there approximations that are differentiable (pref. twice so you can use the very powerful quasi-newton methods)?',\n",
              "       \"is the learning rate related to the shape of the error gradient, asit dictates the rate of descent?in plain sgd, the answer is no. a global learning rate is used which is indifferent to the error gradient. however, the intuition you are getting at has inspired various modifications of the sgd update rule.if so, how do you use this information to inform your decision about a value?adagrad is the most widely known of these and scales a global learning rate η on each dimension based on l2 norm of the history of the error gradient gt on each dimension:adadelta is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of not having to set a learning rate at all.if it's not what sort of values should i choose, and how should i choose them?setting learning rates for plain sgd in neural nets is usually aprocess of starting with a sane value such as 0.01 and then doing cross-validationto find an optimal value. typical values range over a few orders ofmagnitude from 0.0001 up to 1.it seems like you would want small values to avoid overshooting, buthow do you choose one such that you don't get stuck in local minimaor take too long to descend? does it make sense to have a constant learning rate, or should i use some metric to alter its value as i get nearer a minimum in the gradient?usually, the value that's best is near the highest stable learningrate and learning rate decay/annealing (either linear orexponentially) is used over the course of training. the reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.\",\n",
              "       \"i am very new to machine learning and in my first project have stumbled across a lot of issues which i really want to get through.i'm using logistic regression with r's glmnet package and alpha = 0 for ridge regression.i'm using ridge regression actually since lasso deleted all my variables and gave very low area under curve (0.52) but with ridge there isn't much of a difference (0.61).my dependent variable/output is probability of click, based on if there is a click or not in historical data.the independent variables are state, city, device, user age, user gender, ip carrier, keyword, mobile manufacturer, ad template, browser version, browser family, os version and os family.of these, for prediction i'm using state, device, user age, user gender, ip carrier, browser version, browser family, os version and os family; i am not using keyword or template since we want to reject a user request before deep diving in our system and selecting a keyword or template. i am not using city because they are too many or mobile manufacturer because they are too few.is that okay or should i be using the rejected variables?to start, i create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.after training the model, i save the coefficients and intercept. these are used for new incoming requests using the formula for logistic regression:  where a is intercept, k is the ith coefficient and x is the ith variable value.is my approach correct so far?simple glm in r (that is where there is no regularized regression, right?) gave me 0.56 auc. with regularization i get 0.61 but there is no distinct threshold where we could say that above 0.xx its mostly ones and below it most zeros are covered; actually, the max probability that a click didn't happen is almost always greater than the max probability that a click happened.so basically what should i do?i have read how stochastic gradient descent is an effective technique in logit so how do i implement stochastic gradient descent in r? if it's not straightforward, is there a way to implement this system in python? is sgd implemented after generating a regularized logistic regression model or is it a different process altogether?also there is an algorithm called follow the regularized leader (ftrl) that is used in click-through rate prediction. is there a sample code and use of ftrl that i could go through?\",\n",
              "       'if you do not have a gradient available, but the problem is convex, you can use the nelder-mead simplex method. it is available in most optimization packages, for example in scipy.optimize.',\n",
              "       'to complement what insys said :regularization is used when computing the backpropagation for all the weights in your mlp.therefore, instead of computing the gradient in regard to all the input of the training set (batch) you only use some/one item(s) (stochastic or semi-stochastic).you will end up limiting a result of the update in regard to one item instead of all which is also correct.also, if i remember correctly, andrew ng used l2-regularization.the /n in lambda * current_weight / n is not mandatory, it just helps rescaling the input. however if you choose not to use it, you will have (in most of the case) to select another value for lambda.you can also use the grid-search algorithm to choose the best value for lambda (the hyperparameter => the one you have to choose).'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How you can improve this approach ? "
      ],
      "metadata": {
        "id": "3AcnQSRd7XJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here"
      ],
      "metadata": {
        "id": "Fi4Y1vsqB_Sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic similarity"
      ],
      "metadata": {
        "id": "hWNZJR6qYwcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are NLP methods that go further than word-by-word study, by taking into account the context of the terms. There are several methods: Word2vec, Bert.\n",
        "\n",
        "From the Sentence Transformers documentation: https://www.sbert.net/docs/pretrained_models.html choose the pre-trained model that you think is the most appropriate. Justify your choice."
      ],
      "metadata": {
        "id": "6hbMp61saeBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_transformer_model = ''"
      ],
      "metadata": {
        "id": "gOLIblKWCSQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "8O6QZfgPCKMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠ To use Sentence Transformers it is recommended to activate the GPU of google colab."
      ],
      "metadata": {
        "id": "6r2keNXSeQ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ST = SentenceTransformer(sentence_transformer_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "02cbfb1363ca47c3a7cc2999def08213",
            "4bb730174d4c4bbba4541378c4930b9a",
            "fd00d13717a2408d80a03cec7763b295",
            "daf36293a2a14b7f813cb5b5d647e418",
            "e6f675dc25a24500828fb93d771094d7",
            "82f969816ad447da859a3e6783eb91ad",
            "930f1f1678a24ae99b1ddf2e7f20fbb7",
            "ca8f9ce32124466c89bccc4565400592",
            "d557ff364a614615b0e699f6aec4c566",
            "1395a88926674243bcdedfb7d7275cc0",
            "8eb72fa3f46c42139fbc64a1aa08de33",
            "c31e7058036f4a5ab22a274c8ce1a6d1",
            "03232d89579f48a1a5e136585c9fde00",
            "d25d0d63d7e5478bac5f9dac00c1461e",
            "dda5d5c2dbf449e7963326f640853177",
            "9e71b3c469f046038afc7b2f7e3382ba",
            "abd43201cd1c4f7db45ddd229e6b14d7",
            "12bc320bc3af4f06a936c9a6265d377f",
            "4b3b5be0dc5c445b9a3050a1265d2b8b",
            "bf39cbc5495e4078a9ccbaa89ad89334",
            "20544ec0905344e9b2ade3b158e555b4",
            "9858c722a79f447f8c36a6e123531dc0",
            "7bafbc4f65d34c47bc032036ecad79be",
            "9f9e6718289f4b009e5cb4f36ca7bafb",
            "116a657b635243f1ba36a7972c7e8c7e",
            "f229cd2f2ccb429d8eff17791ae35e17",
            "b11de02b306d44e5a260b2d877473f5f",
            "b326af1bb0914bcd952c9dff5abe5d42",
            "579078e1423c48a681adc82059d847f8",
            "bb3564416a644d59a32b8d736f7e04a9",
            "2914a856431c4bf283304e29e9b01ed5",
            "620f8475e644452b87015d8eec37ebb0",
            "5e03079e6aa040c996a4bd2068b6c649",
            "9d92c18a3e06442db95bdb369c052f05",
            "d96e543eb74f48c6a5a5196505b60628",
            "8bee8bb002874630b335d8e0a8b801fe",
            "df4d9fb802124a6f9b9614eb3673bcf7",
            "4d9101a4b6024a39a8507ff585af1259",
            "8780b3fc6e7243f1a63c7c94fbdd8053",
            "d85ece8d99bb49139a70e016a06f97f1",
            "20281614a52b4fa7a93cddd31092adc5",
            "6ace7e1eaf8b451499dc60cbcc78d22e",
            "1daef32e6b51422eb47f135fdc8150f7",
            "799707ac4d0540a7ac88c80d95cc50fa",
            "a91b586064e740feb8eeb27c02590ccf",
            "a04d6415638344878fe8246ea326ffb0",
            "a1ed9c74fac441c9ace0aea654b3ae17",
            "f7d276d2f26041509e568c41e0f30420",
            "756d548f4c64446eb41d891c99b71c75",
            "3ad60474bdfc4b8fac306b1266371e9a",
            "ab54cc66da5f492a9830ca25f5f5acc7",
            "021c85fabd9047a493eb838440a46430",
            "73706f650c7e4ecd923fd51f5e1a714e",
            "890caa151ead4c24a8ef52684b4d1936",
            "66212781471e4992aa8ad74d52cd6884",
            "ee4622ffc57b421d8ab54a678bf524c3",
            "34adba70a0ec49e2ab2c678bff13f7a3",
            "f9b5207a40ee43bfaac71a93ef3fbde8",
            "e2cb4079b5e5401799fd1a63b4ffa04a",
            "d7d995cd240f4986b80abfe88b3274d6",
            "aae788b1c20645138d1d2d40d238a8aa",
            "4aca7ebf6cca4e08ad5becd75fbb9998",
            "89fb3553f50b457eb0894ed332e23f92",
            "10c179a861ab4e97b3d12c5dae4e1cb4",
            "f76299c226814ecb889be4fff0a0a028",
            "df42d059791b419cab83b20391557492",
            "7aa7c814c69e4f30ab54c7fed798031f",
            "c4adea87965f4f5ebb14058c6a94923f",
            "61f154962b844c809845f92091a16bf6",
            "0e23fa3ddec14f5699ede7e0dd685918",
            "ed9ae88080e14fc7a91af2cf8fba6a99",
            "edc62f56743a4b559e319134e994cbe8",
            "91b1d43e7b0f41729b62518b771239d8",
            "91926dcd6b9547b39d159e1169034f1c",
            "75c8821cea98493ab7aebeb5623131d6",
            "4463494d599648538e82ec1f0e908373",
            "a718f09041474a56a5ee86153f4f3bf3",
            "2ab01b6399ef40fba0e62527ccfe2b0c",
            "6558d1e3fd4b4cf291d83f5c3361caee",
            "0d1614f5f28b4673a99fc636195b44a6",
            "0613301aee514059be98d14668f1c5f1",
            "ce1d4f6b299344f5abed0651673ddf4a",
            "ac706715d0bb43e1b7c4723c9e5e4430",
            "b8ae662ba8cb468e9d51583ebec724b5",
            "a3f3dad3d24249269a74ba5f926c5dad",
            "1d400d2d61e94178ba7a10ecf88e8122",
            "464ab9096f6d4a3cb22c798aecaf389a",
            "3334558c60c34242aa337416918e5a61",
            "000421c2dae04292b304b5821cb1da18",
            "5d6ca948e1fb45c7874cfecd67baf42b",
            "8cfa043308094af1964a815860d4e6a9",
            "4480b9f250af492ca3855daa3a71a754",
            "1c98baf7eac4438d8e9a156b09c61e3d",
            "98c895752ff94e1ab0b9a5a650a55bd8",
            "9032c75a1b354ce8bda731535f6153d8",
            "99800bb0b202471081130fdd1ff908f5",
            "d6a7aada7e484e65a0027ff39fb0d160",
            "db1811b4dd6f449b8c4262382714405b",
            "b87e07ba46dd42e9966209ea70d0a056",
            "43fea82a481b4ecbb1ff5f230e8cc892",
            "8dc77074483f4aef97e47e6a04825cc5",
            "903f969ae3284dba85cec6e0c02f63c2",
            "7618bac4a10d4e73a3e584ad3d4709e8",
            "911c9cceca994aafa1807946894e15a1",
            "e4309448f07c458988dfec0059dbb060",
            "f87cdb0434dd4015bd0247cd8f76c44b",
            "d84f51b720b94d68ba6e217de01862a2",
            "30b2d058d9cd4fbf9a21a5a59e49a5d0",
            "45aeb8a8844f4bc4969f6658ee46eedf",
            "06e9b832566245e4aaab7267a966b303",
            "9ae513e627704134ad081e1dea561753",
            "b6d07ce384e74789869a47985a43f59e",
            "891bd71b9c654ac185c2d2642f40cfce",
            "6772b814e22f48c0935c78c4e0473bce",
            "8f9dc7eb45004f10b897bc97c808d0ce",
            "724890f8bb7248bf8b657c5cf9474d8a",
            "fccb0f67b73f4545a898bef4595e8c28",
            "3d6eb5d04dbb4601ac4dfdeb020fed99",
            "94d7d2d5cd194dc78ff9e39d3d3805f6",
            "ceb2ab98fa9d46189cef411c7078ffcf",
            "a298370ee5ab4396966d1ad4ab4f3fb8",
            "1d277ba2fc9a4223a7b1f5b5ad9a810c",
            "aaf6f82410c3424286f243d7c23086a2",
            "e46fea149dad4c638cc5059498a5e72b",
            "e0ada647f63a46c5b233a56a7531b6d8",
            "1e32c7f8c2594c95bb71a750ee4e9d54",
            "ff3788b121ca499fb214d9c281215798",
            "cf9800f0b7af4cafa11862cfbae61bdf",
            "e5a4b46cf4324b4a9d6b7c0e3a43d5cc",
            "13e9c182313340bfa06aba7b26450dfe",
            "d2bcf14afe9e49d786143eb6f426d391",
            "e5b40b520c194eb3ac4e210054acfaac",
            "fafef24076c748fbb90b5493742fe60f",
            "531e17804a964eed8cbf0026be12a6a1",
            "e934ec0c85a941dba7bc9639e9954d63",
            "214b5c6307c24bc9b48f5af649381866",
            "0036236066614fa4a3e41edb26fd145e",
            "26b7262aa5b54d3db86a5e98f4cb42de",
            "8107de17103f43c3bdb363ae5e8c0345",
            "62bd6da8a62c40129d2df54709717982",
            "8d45f5c29b6a454085d8c0993ab4920d",
            "a553b7100609441c812b6da9f0dfaea6",
            "b3d38e70255846a7b9dc25e092a89322",
            "143eb44c074f4037a4eeb61be9d52517",
            "7c9ecabc3db3498ba32fe820ce34ca8a",
            "7cc1d25c4d1c4365a73eca3584959c67",
            "3ebc5279727b4f578dd55bd34e5b7f47",
            "9b69e77883e14a18935a48b1c1d62275",
            "2867eb3ec54949fca3b840e3f8c1b191",
            "446d5dfb2a3f43b196dad2104f859102",
            "678fa128e70745509675063c2bbe37a6",
            "47f6af648231461e8bea7026342f7269",
            "2cb298973e6244db94a4e66e1065eda5",
            "089846d096e0463fa15075a1a341d378"
          ]
        },
        "id": "dCTzpEkkeQAO",
        "outputId": "6c8ec330-3745-41fa-c3fe-bf327ac6f405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02cbfb1363ca47c3a7cc2999def08213"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c31e7058036f4a5ab22a274c8ce1a6d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bafbc4f65d34c47bc032036ecad79be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d92c18a3e06442db95bdb369c052f05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a91b586064e740feb8eeb27c02590ccf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee4622ffc57b421d8ab54a678bf524c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aa7c814c69e4f30ab54c7fed798031f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ab01b6399ef40fba0e62527ccfe2b0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "000421c2dae04292b304b5821cb1da18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43fea82a481b4ecbb1ff5f230e8cc892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ae513e627704134ad081e1dea561753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d277ba2fc9a4223a7b1f5b5ad9a810c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fafef24076c748fbb90b5493742fe60f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "143eb44c074f4037a4eeb61be9d52517"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this algorithm to encode the data in the database"
      ],
      "metadata": {
        "id": "Ph24xTeVgTpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = MODEL_ST.encode(posts.cleaned_body.values, normalize_embeddings=True)"
      ],
      "metadata": {
        "id": "v-TTENiM5vn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*If this process is slow, you can save this array in case you need to load it again*"
      ],
      "metadata": {
        "id": "TBJ0W5UygN9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(os.path.join(DATA_PATH, 'embeddings.pkl', 'wb') as file:\n",
        "    pickle.dump(embeddings, file)"
      ],
      "metadata": {
        "id": "Qkw285npGxmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a function that transforms the input"
      ],
      "metadata": {
        "id": "2Hy__GOwgbIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_query(query : str) ->  numpy.ndarray:\n",
        "    #TODO\n",
        "    return encoded_query"
      ],
      "metadata": {
        "id": "35As3cuYgRWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which distance is most relevant to measure the distance between the input and the data?"
      ],
      "metadata": {
        "id": "xFD6pJIpgEz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here"
      ],
      "metadata": {
        "id": "2XHL8gFRIUu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that returns a matrix containing information about the similarity between the query and the data"
      ],
      "metadata": {
        "id": "6LxvJiVKIv-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(query, embeddings=embeddings):\n",
        "    #TODO\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "ZtDl3Cicf9wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_similarity = similarity(query, embeddings)"
      ],
      "metadata": {
        "id": "_oR0xnZCJKfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do you determine which documents in the data set most closely match the input?"
      ],
      "metadata": {
        "id": "2dRGokvThOcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ordre_en_fonction_similarité(matrix_similarity):\n",
        "    #TODO\n",
        "    return ordre"
      ],
      "metadata": {
        "id": "jd1UD26VhN_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert ordre_en_fonction_similarité([0.6, 0.8, 0.7]) == [1, 2, 0]"
      ],
      "metadata": {
        "id": "9-CoY6VUj5EE",
        "outputId": "81ec6aa6-f24b-478f-c70a-6f85ae8ac78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put it all together in a function."
      ],
      "metadata": {
        "id": "a3IBMm8HisWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closest_semantic_doc(query, embeddings=embeddings, top_n=10):\n",
        "    return closest_posts"
      ],
      "metadata": {
        "id": "BOFygRpbifXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What methods could be used to improve the recommendations of this algorithm?"
      ],
      "metadata": {
        "id": "ouUjQrhgjEDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here"
      ],
      "metadata": {
        "id": "_v7HI2pDJk-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#todo"
      ],
      "metadata": {
        "id": "vNP-BErhJm4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text clustering (BONUS)"
      ],
      "metadata": {
        "id": "fa4Ij2usk8Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use topic modeling techniques to identify groups of texts among our document base and classify the input to restrict the application of the proximity calculations seen previously."
      ],
      "metadata": {
        "id": "EfZcQyxLox8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LDA"
      ],
      "metadata": {
        "id": "kt9_ujt6sVMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Dirichlet Allocation is a topic modeling algorithm that allows soft clustering. Soft clustering means that the LDA does not allocate an input to a cluster, but gives a probabilistic score for each identified cluster. This decomposition allows to identify topics within the documents. \n",
        "\n",
        "In order to compute this algorithm, you need to vectorize your data (you can use the one you have already done previously or make another one)."
      ],
      "metadata": {
        "id": "eH2IG1iyTxiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize document using TF-IDF\n",
        "vectorizer_lda = \n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = vectorizer_lda.fit_transform(posts.cleaned_body.values)"
      ],
      "metadata": {
        "id": "BfpEOZYkk7kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use Gensim or scikit-learn to compute LDA."
      ],
      "metadata": {
        "id": "WSZ1kI3JWfTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "oDGFnU1CJ6_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign a main topic to each document"
      ],
      "metadata": {
        "id": "nG_grQ93tMIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_documents = "
      ],
      "metadata": {
        "id": "ZM3cSJrWuYT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that assigns a topic to the query\n"
      ],
      "metadata": {
        "id": "oO1ZaPy6s6Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_query(query, vectorizer=vectorizer_lda, lda_model=lda_model) -> int:\n",
        "    #TODO\n",
        "    return topic_query"
      ],
      "metadata": {
        "id": "n86QrUuus5Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge methods"
      ],
      "metadata": {
        "id": "ZQfMqiMpvKk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write an algorithm to merge the methods seen. Which methods to use? How can you check if they are relevant ?"
      ],
      "metadata": {
        "id": "Ll1RCEV-vVbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here"
      ],
      "metadata": {
        "id": "-vIibmanKPLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nlp_search_algorithm(query,\n",
        "                         topic_documents=topic_documents,\n",
        "                         vectors=vectors,\n",
        "                         vectorizer=vectorizer,\n",
        "                         vectorizer_lda=vectorizer_lda,\n",
        "                         lda_model=lda_model,\n",
        "                         embeddings=embeddings,\n",
        "                         top_n=10\n",
        "                         )->list:\n",
        "    #TODO\n",
        " \n",
        "    return matching_posts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v2GUl3uHSJej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have a list of possible results, you can it: (you can use one of the ranking algorithms you have previously done or make up a new one)"
      ],
      "metadata": {
        "id": "mWh3EuXiT_Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank(possible_results):\n",
        "    #to_do\n",
        "    return possible_results"
      ],
      "metadata": {
        "id": "9FbEcpONT-1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incorporation in the search engine"
      ],
      "metadata": {
        "id": "pJ3vhg1kpX6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addition of metadata"
      ],
      "metadata": {
        "id": "4CDbNI-qpf3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must now have a new set of metadata and data to add to your original index. You can load the index you had as a result of Day 2 and today's work to it. "
      ],
      "metadata": {
        "id": "yZBXyHN3MKBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load previous data \n",
        "\n",
        "#TODO "
      ],
      "metadata": {
        "id": "dK9U-NB0MctD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the new data to the previous index"
      ],
      "metadata": {
        "id": "yV6jul7jNW7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint : if you have changed the preprocessing function at the beginning of this notebook make sure to update the Clean Body attribute"
      ],
      "metadata": {
        "id": "HvDxs_EZKyeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaptation to the index format\n",
        "\n",
        "Adapt your nlp search results to the index format"
      ],
      "metadata": {
        "id": "AZ8PuvLBUap8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nlp_search_in_index(query,\n",
        "                        index,\n",
        "                        args):\n",
        "\n",
        "    return matching_posts\n",
        "  "
      ],
      "metadata": {
        "id": "alY_kdTOUaFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare the new searching and ranking method to the previous ones\n",
        "\n",
        "Compare in terms of efficiency (precision, completeness, speed, memory usage)"
      ],
      "metadata": {
        "id": "vPC4lvgeUpAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge all methods to make an efficient search algorithm"
      ],
      "metadata": {
        "id": "N_yte-oapjFW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGcqIGl8pXhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CU2e3tgiIHzr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}